{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k6uMuiEGyoW"
      },
      "source": [
        "# Preprocess\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5nEyGbpx4mC"
      },
      "source": [
        "## Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziJ6NvN-x3hi"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_directory = f'/content/drive/MyDrive/Analisis-Sentimen-CNBC'\n",
        "dataset_file_name = 'dataset'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E27w3XqNxttL"
      },
      "source": [
        "## Cleaning & Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "em2WfHxcxqyD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Inisialisasi tokenizer IndoBERT\n",
        "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-large-p2')\n",
        "\n",
        "# Baca dataset\n",
        "df = pd.read_csv(f'{base_directory}/Dataset/{dataset_file_name}.csv')  # Sesuaikan path dataset Anda\n",
        "\n",
        "def smart_lowercase(text):\n",
        "    tokens = text.split()\n",
        "    result = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if token.isupper() and 2 <= len(token) <= 6:\n",
        "            result.append(token)\n",
        "        else:\n",
        "            result.append(token.lower())\n",
        "\n",
        "    return ' '.join(result)\n",
        "\n",
        "def preprocess(text):\n",
        "    text = smart_lowercase(text)\n",
        "    text = re.sub(r'(\\d+)\\.(\\d+)[ -]?an', r'\\1\\2-an', text)\n",
        "    text = re.sub(r'(\\d+),(\\d+)%', r'\\1.\\2%', text)\n",
        "    text = re.sub(r'(\\bUS|\\bUSD)\\$', r'\\1 $ ', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'(rp)(\\d+),(\\d+)\\s*([tmk])', r'\\1 \\2.\\3\\4', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'(rp)(\\d+),(\\d+)', r'\\1 \\2.\\3', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'(rp)(\\d+)', r'\\1 \\2', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'(\\d+),(\\d+)', r'\\1.\\2', text)\n",
        "    text = re.sub(r'(\\d+\\.\\d+)([tmk])', r'\\1 \\2', text)\n",
        "    text = re.sub(r'(?<=[^\\d])([.,!?()\":])|([.,!?()\":])(?=[^\\d])', r' \\1\\2 ', text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9.,!?()%\":\\s-]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Preprocess dan Tokenize\n",
        "texts_cleaned = []\n",
        "tokens_cleaned = []\n",
        "\n",
        "for text in df['judul']:\n",
        "    cleaned = preprocess(text)\n",
        "    texts_cleaned.append(cleaned)\n",
        "    tokens_cleaned.append(tokenizer.tokenize(cleaned))\n",
        "\n",
        "# Tambahkan ke dataframe\n",
        "df.insert(df.columns.get_loc('judul') + 1, 'text', texts_cleaned)\n",
        "df.insert(df.columns.get_loc('text') + 1, 'tokens', tokens_cleaned)\n",
        "\n",
        "# Simpan hasil\n",
        "output_path = f'{base_directory}/Dataset/{dataset_file_name}-preprocessed.csv'\n",
        "df.to_csv(output_path, index=False)\n",
        "print(\"‚úÖ File berhasil disimpan dengan tokenisasi IndoBERT di kolom `tokens`!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split with Jaccard Similiarity"
      ],
      "metadata": {
        "id": "FWbm66vBZuE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ~/.cache/pip\n",
        "\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "FBcQo9cKjIcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import os\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# === 1. Load Dataset ===\n",
        "df = pd.read_csv(f'{base_directory}/Dataset/{dataset_file_name}-preprocessed.csv')\n",
        "\n",
        "# === 2. Setup Split Ratio ===\n",
        "test_size = 0.3  # Ubah sesuai rasio test yang diinginkan\n",
        "train_size = int((1 - test_size) * 100)\n",
        "validation_size = int(test_size * 100)\n",
        "n_splits = 100  # jumlah percobaan random split\n",
        "\n",
        "# === 3. Load Tokenizer IndoBERT ===\n",
        "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-large-p2')\n",
        "\n",
        "# === 4. Tokenisasi dan Jaccard Similarity ===\n",
        "def get_token_set(token_lists):\n",
        "    tokens = set()\n",
        "    for token_list in token_lists:\n",
        "        tokens.update(eval(token_list))  # karena disimpan sebagai string list saat CSV disimpan\n",
        "    return tokens\n",
        "\n",
        "def jaccard_similarity(a, b):\n",
        "    return len(a & b) / len(a | b) if (a | b) else 0\n",
        "\n",
        "# === 5. Lakukan Stratified Shuffle Split + Pilih yang Similarity Tertinggi ===\n",
        "X = df['text']\n",
        "y = df['sentimen']\n",
        "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=42)\n",
        "\n",
        "best_similarity = 0\n",
        "best_split = None\n",
        "best_iter = 0\n",
        "\n",
        "print(f\"üîÅ Mulai proses {n_splits} kali Stratified Shuffle Split...\\n\")\n",
        "\n",
        "for i, (train_idx, val_idx) in enumerate(sss.split(X, y), start=1):\n",
        "    print(f\"üîÑ Iterasi {i}/{n_splits} ...\", flush=True)\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_tokens = get_token_set(df.iloc[train_idx]['tokens'])\n",
        "    val_tokens = get_token_set(df.iloc[val_idx]['tokens'])\n",
        "    sim_token = jaccard_similarity(train_tokens, val_tokens)\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "\n",
        "    print(f\"‚úÖ Iterasi {i} selesai ‚Äî Jaccard Similarity: {sim_token:.3f} ‚Äî Waktu: {duration:.2f} detik\\n\", flush=True)\n",
        "\n",
        "    if sim_token > best_similarity:\n",
        "        best_similarity = sim_token\n",
        "        best_split = (train_idx, val_idx)\n",
        "        best_iter = i\n",
        "\n",
        "# === 6. Simpan Data Split Terbaik ===\n",
        "train_data = df.iloc[best_split[0]].reset_index(drop=True)\n",
        "validation_data = df.iloc[best_split[1]].reset_index(drop=True)\n",
        "\n",
        "# Hapus kolom 'tokens' setelah split\n",
        "train_data = train_data.drop(columns=['tokens'])\n",
        "validation_data = validation_data.drop(columns=['tokens'])\n",
        "\n",
        "save_path = f'{base_directory}/Dataset/{train_size}-{validation_size}'\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "train_data.to_csv(f'{save_path}/train_data.csv', index=False)\n",
        "validation_data.to_csv(f'{save_path}/validation_data.csv', index=False)\n",
        "\n",
        "# === 7. Buat Laporan Split ===\n",
        "def label_distribution(series):\n",
        "    counter = Counter(series)\n",
        "    return '\\n'.join([f\"  - {label} : {count}\" for label, count in counter.items()])\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "report = f\"\"\"\n",
        "üìä Split Report\n",
        "\n",
        "Waktu Eksekusi        : {timestamp}\n",
        "Split Terbaik (Iterasi): {best_iter}\n",
        "Jaccard Similarity     : {best_similarity:.3f}\n",
        "\n",
        "Data Training          : {len(train_data)} rows\n",
        "Data Validation        : {len(validation_data)} rows\n",
        "\n",
        "Distribusi Label (Train):\n",
        "{label_distribution(train_data['sentimen'])}\n",
        "\n",
        "Distribusi Label (Validation):\n",
        "{label_distribution(validation_data['sentimen'])}\n",
        "\"\"\"\n",
        "\n",
        "with open(f'{save_path}/split_report.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(report.strip())\n",
        "\n",
        "# === 8. Print Ringkasan ===\n",
        "print(\"‚úÖ Split data terbaik ditemukan menggunakan StratifiedShuffleSplit!\")\n",
        "print(f\"üìÅ Folder disimpan di     : {save_path}\")\n",
        "print(f\"üìÑ Laporan split          : {save_path}/split_report.txt\")\n",
        "print(f\"üìà Best Token Similarity  : {best_similarity:.3f}\")\n"
      ],
      "metadata": {
        "id": "zMWug4KhZqtX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}