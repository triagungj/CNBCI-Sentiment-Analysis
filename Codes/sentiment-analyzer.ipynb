{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tge3YjUkDPdq"
      },
      "source": [
        "# Install & Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gIcyYilrkLj"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoD-GGK_DFcU"
      },
      "outputs": [],
      "source": [
        "# Install\n",
        "!pip install -r https://raw.githubusercontent.com/IndoNLP/indonlu/master/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEQr50TGrnK8"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWwpSDSGmHwF"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, classification_report\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14SBhI74el16"
      },
      "source": [
        "# Define classes & function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWpPDQzEhUQw"
      },
      "source": [
        "## Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JifThxcumNTP"
      },
      "outputs": [],
      "source": [
        "# Classes\n",
        "class DocumentSentimentDataLoader(DataLoader):\n",
        "    def __init__(self, max_seq_len=512, *args, **kwargs):\n",
        "        super(DocumentSentimentDataLoader, self).__init__(*args, **kwargs)\n",
        "        self.collate_fn = self._collate_fn\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def _collate_fn(self, batch):\n",
        "        batch_size = len(batch)\n",
        "        max_seq_len = max(map(lambda x: len(x[0]), batch))\n",
        "        max_seq_len = min(self.max_seq_len, max_seq_len)\n",
        "\n",
        "        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
        "        mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)\n",
        "        sentiment_batch = np.zeros((batch_size, 1), dtype=np.int64)\n",
        "\n",
        "        seq_list = []\n",
        "        for i, (subwords, sentiment, raw_seq) in enumerate(batch):\n",
        "            subwords = subwords[:max_seq_len]\n",
        "            subword_batch[i,:len(subwords)] = subwords\n",
        "            mask_batch[i,:len(subwords)] = 1\n",
        "            sentiment_batch[i,0] = sentiment\n",
        "\n",
        "            seq_list.append(raw_seq)\n",
        "\n",
        "        return subword_batch, mask_batch, sentiment_batch, seq_list\n",
        "\n",
        "class DocumentSentimentDataset(Dataset):\n",
        "    # Static constant variable\n",
        "    LABEL2INDEX = {'positif': 0, 'netral': 1, 'negatif': 2}\n",
        "    INDEX2LABEL = {0: 'positif', 1: 'netral', 2: 'negatif'}\n",
        "    NUM_LABELS = 3\n",
        "\n",
        "    def load_dataset(self, path):\n",
        "        # Baca dataset dari file CSV\n",
        "        df = pd.read_csv(path)\n",
        "\n",
        "        # Pastikan kolom yang dibaca sesuai dengan dataset Anda\n",
        "        df.columns = ['judul', 'text', 'tanggal', 'sentimen']\n",
        "\n",
        "        # Ubah label sentimen menjadi indeks\n",
        "        df['sentimen'] = df['sentimen'].apply(lambda lab: self.LABEL2INDEX[lab])\n",
        "        return df\n",
        "\n",
        "    def __init__(self, dataset_path, tokenizer, no_special_token=False, *args, **kwargs):\n",
        "        # Load dataset\n",
        "        self.data = self.load_dataset(dataset_path)\n",
        "\n",
        "        # Tokenizer untuk encoding teks\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Opsi untuk menambahkan special token atau tidak\n",
        "        self.no_special_token = no_special_token\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Ambil data pada indeks tertentu\n",
        "        data = self.data.loc[index, :]\n",
        "\n",
        "        # Ambil judul dan sentimen\n",
        "        text, sentiment = data['text'], data['sentimen']\n",
        "\n",
        "        # Encode teks menjadi subwords\n",
        "        subwords = self.tokenizer.encode(text, add_special_tokens=not self.no_special_token)\n",
        "\n",
        "        # Return subwords, sentiment, dan judul (untuk referensi)\n",
        "        return np.array(subwords), np.array(sentiment), data['text']\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return jumlah data dalam dataset\n",
        "        return len(self.data)\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, verbose=True, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): Berapa banyak epoch tanpa perbaikan sebelum berhenti.\n",
        "            verbose (bool): Cetak info saat validasi loss membaik.\n",
        "            delta (float): Perubahan minimum yang dianggap perbaikan.\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.delta = delta\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.counter = 0\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, prev_best=None)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            prev_best = -self.best_score  # Simpan nilai terbaik sebelum update\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, prev_best)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, prev_best=None):\n",
        "        '''Simpan model ketika validasi loss membaik.'''\n",
        "        if self.verbose:\n",
        "            if prev_best is None:\n",
        "                print(f'Validation loss initialized: {val_loss:.6f}. Saving model...')\n",
        "            else:\n",
        "                print(f'Validation loss decreased ({prev_best:.6f} --> {val_loss:.6f}). Saving model...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrhunam9hYo8"
      },
      "source": [
        "## Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neD_D6Fmxc37"
      },
      "outputs": [],
      "source": [
        "# Forward function for sequence classification\n",
        "def forward_sequence_classification(model, batch_data, i2w, is_test=False, device='gpu', **kwargs):\n",
        "    \"\"\"\n",
        "    Forward function for sequence classification.\n",
        "\n",
        "    Args:\n",
        "        model: Model yang akan digunakan.\n",
        "        batch_data: Data batch yang berisi subword, mask, token_type (opsional), dan label.\n",
        "        i2w: Mapping dari indeks ke label (untuk decoding prediksi).\n",
        "        is_test: Flag untuk menentukan apakah ini tahap testing (default: False).\n",
        "        device: Perangkat yang digunakan ('cuda' atau 'cpu').\n",
        "        **kwargs: Argumen tambahan.\n",
        "\n",
        "    Returns:\n",
        "        loss: Loss dari model.\n",
        "        list_hyp: List prediksi.\n",
        "        list_label: List label sebenarnya.\n",
        "    \"\"\"\n",
        "    # Unpack batch data\n",
        "    if len(batch_data) == 3:\n",
        "        (subword_batch, mask_batch, label_batch) = batch_data\n",
        "        token_type_batch = None\n",
        "    elif len(batch_data) == 4:\n",
        "        (subword_batch, mask_batch, token_type_batch, label_batch) = batch_data\n",
        "\n",
        "    # Prepare input & label\n",
        "    subword_batch = torch.LongTensor(subword_batch)\n",
        "    mask_batch = torch.FloatTensor(mask_batch)\n",
        "    token_type_batch = torch.LongTensor(token_type_batch) if token_type_batch is not None else None\n",
        "    label_batch = torch.LongTensor(label_batch)\n",
        "\n",
        "    # Pindahkan tensor ke device yang sesuai\n",
        "    if device == \"cuda\":\n",
        "        subword_batch = subword_batch.cuda()\n",
        "        mask_batch = mask_batch.cuda()\n",
        "        token_type_batch = token_type_batch.cuda() if token_type_batch is not None else None\n",
        "        label_batch = label_batch.cuda()\n",
        "    else:\n",
        "        subword_batch = subword_batch.cpu()\n",
        "        mask_batch = mask_batch.cpu()\n",
        "        token_type_batch = token_type_batch.cpu() if token_type_batch is not None else None\n",
        "        label_batch = label_batch.cpu()\n",
        "\n",
        "    # Forward model\n",
        "    outputs = model(subword_batch, attention_mask=mask_batch, token_type_ids=token_type_batch, labels=label_batch)\n",
        "    loss, logits = outputs[:2]\n",
        "\n",
        "    # Generate prediction & label list\n",
        "    list_hyp = []\n",
        "    list_label = []\n",
        "    list_confidence = []\n",
        "\n",
        "    hyp = torch.topk(logits, 1)[1]  # Ambil prediksi dengan nilai tertinggi\n",
        "    for j in range(len(hyp)):\n",
        "        list_hyp.append(i2w[hyp[j].item()])  # Decode prediksi\n",
        "        list_label.append(i2w[label_batch[j].item()])  # Decode label sebenarnya\n",
        "        list_confidence.append(F.softmax(logits, dim=1)[j].max().item())  # Ambil nilai confidence\n",
        "\n",
        "    return loss, list_hyp, list_label, list_confidence\n",
        "\n",
        "\n",
        "def document_sentiment_metrics_fn(list_hyp, list_label):\n",
        "    metrics = {}\n",
        "    metrics[\"ACC\"] = accuracy_score(list_label, list_hyp)\n",
        "    metrics[\"F1\"] = f1_score(list_label, list_hyp, average='macro')\n",
        "    metrics[\"REC\"] = recall_score(list_label, list_hyp, average='macro')\n",
        "    metrics[\"PRE\"] = precision_score(list_label, list_hyp, average='macro')\n",
        "    return metrics\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "def count_param(module, trainable=False):\n",
        "    if trainable:\n",
        "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "    else:\n",
        "        return sum(p.numel() for p in module.parameters())\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def metrics_to_string(metric_dict):\n",
        "    string_list = []\n",
        "    for key, value in metric_dict.items():\n",
        "        string_list.append('{}:{:.2f}'.format(key, value))\n",
        "    return ' '.join(string_list)\n",
        "\n",
        "# Define the training function\n",
        "def train_model(model, train_loader, optimizer, epoch, i2w):\n",
        "    model.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "    total_train_loss = 0\n",
        "    list_hyp, list_label, list_confidence = [], [], []\n",
        "\n",
        "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
        "    for i, batch_data in enumerate(train_pbar):\n",
        "        # Forward model\n",
        "        loss, batch_hyp, batch_label, confidence = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "\n",
        "        # Update model\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tr_loss = loss.item()\n",
        "        total_train_loss += tr_loss\n",
        "\n",
        "        # Calculate metrics\n",
        "        list_hyp += batch_hyp\n",
        "        list_label += batch_label\n",
        "        list_confidence += confidence\n",
        "\n",
        "        train_pbar.set_description(f\"(Epoch {epoch+1}) TRAIN LOSS:{total_train_loss/(i+1):.4f} LR:{get_lr(optimizer):.8f}\")\n",
        "\n",
        "    # Calculate train metric\n",
        "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "    print(f\"(Epoch {epoch+1}) TRAIN LOSS:{total_train_loss/(i+1):.4f} {metrics_to_string(metrics)} LR:{get_lr(optimizer):.8f}\")\n",
        "    return list_hyp, list_label, list_confidence, total_train_loss\n",
        "\n",
        "# Define the evaluation function\n",
        "def evaluate_model(model, valid_loader, i2w):\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    total_loss = 0\n",
        "    list_hyp, list_label, list_confidence = [], [], []\n",
        "\n",
        "    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
        "    for i, batch_data in enumerate(pbar):\n",
        "        loss, batch_hyp, batch_label, confidence = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "\n",
        "        valid_loss = loss.item()\n",
        "        total_loss += valid_loss\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        list_hyp += batch_hyp\n",
        "        list_label += batch_label\n",
        "        list_confidence += confidence\n",
        "        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "\n",
        "        pbar.set_description(f\"VALID LOSS:{total_loss/(i+1):.4f} {metrics_to_string(metrics)}\")\n",
        "\n",
        "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "    print(f\"VALID LOSS:{total_loss/(i+1):.4f} {metrics_to_string(metrics)}\")\n",
        "\n",
        "    return list_hyp, list_label, list_confidence, total_loss\n",
        "\n",
        "# Define the test evaluation function\n",
        "def test_model(model, valid_loader, i2w):\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    total_loss = 0\n",
        "    list_hyp, list_label, list_confidence = [], [], []\n",
        "\n",
        "    test_pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
        "    for i, batch_data in enumerate(test_pbar):\n",
        "        loss, batch_hyp, batch_label, confidence = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "\n",
        "        test_loss = loss.item()\n",
        "        total_loss += test_loss\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        list_hyp += batch_hyp\n",
        "        list_label += batch_label\n",
        "        list_confidence += confidence\n",
        "        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "\n",
        "        test_pbar.set_description(f\"TEST LOSS:{total_loss/(i+1):.4f} {metrics_to_string(metrics)}\")\n",
        "\n",
        "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "    print(f\"TEST LOSS:{total_loss/(i+1):.4f} {metrics_to_string(metrics)}\")\n",
        "\n",
        "    return list_hyp, list_label, list_confidence, total_loss\n",
        "\n",
        "# Define the function to generate the confusion matrix and other reports\n",
        "def generate_reports(dataset_path, list_hyp, list_label, list_confidence, dataset_ratio, lr, current_epoch, accuracy, loss, path_name, rename_folder=False):\n",
        "    # Confusion matrix and other evaluations\n",
        "    cm = confusion_matrix(list_label, list_hyp)\n",
        "\n",
        "    # Visualize confusion matrix\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negatif', 'Netral', 'Positif'], yticklabels=['Negatif', 'Netral', 'Positif'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "\n",
        "    # Define result directory\n",
        "    main_dir = f'{base_directory}/Test-Results/{model_name}'\n",
        "    param_name = f'{dataset_ratio}-lr_{lr}'\n",
        "    test_result_dir = f'{main_dir}/{param_name}/{path_name}/epoch_{current_epoch}-acr_{accuracy}-loss_{loss}'\n",
        "    os.makedirs(test_result_dir, exist_ok=True)\n",
        "\n",
        "    # Save confusion matrix image\n",
        "    confusion_matrix_path = f'{test_result_dir}/confusion_matrix.png'\n",
        "    plt.savefig(confusion_matrix_path)\n",
        "    plt.show()\n",
        "\n",
        "    # Save classification report\n",
        "    classification_report_path = f'{test_result_dir}/classification_report.txt'\n",
        "    with open(classification_report_path, 'w') as f:\n",
        "        report = classification_report(list_label, list_hyp, target_names=['Negatif', 'Netral', 'Positif'])\n",
        "        f.write(report)\n",
        "\n",
        "    # Save confusion matrix and classification report to a .txt file\n",
        "    evaluation_results_path = f'{test_result_dir}/evaluation_results.txt'\n",
        "    with open(evaluation_results_path, 'a') as f:\n",
        "        f.write(\"=== Confusion Matrix ===\\n\")\n",
        "        f.write(np.array2string(cm))\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "        f.write(\"=== Classification Report ===\\n\")\n",
        "        f.write(classification_report(list_label, list_hyp, target_names=['Negatif', 'Netral', 'Positif']))\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "\n",
        "    # Save actual labels and predictions to a CSV file\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    df['prediksi'] = list_hyp\n",
        "    df['confidence'] = list_confidence\n",
        "    # Tambahkan kolom 'needs_review' jika label manual beda atau confidence < threshold\n",
        "    df['needs_review'] = (\n",
        "        (df['sentimen'] != df['prediksi']) | (df['confidence'] < 0.90)\n",
        "    )\n",
        "\n",
        "    # Tambahkan kolom 'overconfident_errors' jika prediksi salah tapi confidence tinggi\n",
        "    df['overconfident_errors'] = (\n",
        "        (df['sentimen'] != df['prediksi']) & (df['confidence'] > 0.9)\n",
        "    )\n",
        "    predictions_file_path = f'{test_result_dir}/predictions.csv'\n",
        "    df.to_csv(predictions_file_path, index=False, encoding='utf-8')\n",
        "\n",
        "    if rename_folder:\n",
        "      os.rename(f'{main_dir}/{param_name}', f'{main_dir}/acr_{accuracy}-{param_name}')\n",
        "\n",
        "    print(f\"Confusion matrix saved to: {confusion_matrix_path}\")\n",
        "    print(f\"Classification report saved to: {classification_report_path}\")\n",
        "    print(f\"Evaluation results saved to: {evaluation_results_path}\")\n",
        "    print(f\"Predictions saved to: {predictions_file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5OHo6QkmlFG"
      },
      "source": [
        "# Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-on6x7omjGW"
      },
      "outputs": [],
      "source": [
        "base_directory = f'/content/drive/MyDrive/Analisis-Sentimen-CNBC'\n",
        "\n",
        "model_name = 'indobert-large-p2'\n",
        "\n",
        "learning_rate = [\n",
        "    2e-5,\n",
        "    2e-6,\n",
        "    3e-5,\n",
        "    3e-6\n",
        "]\n",
        "dataset_ratios = [\n",
        "    '90-10',\n",
        "    '80-20',\n",
        "    '70-30'\n",
        "]\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(f'indobenchmark/{model_name}')\n",
        "config = BertConfig.from_pretrained(f'indobenchmark/{model_name}')\n",
        "config.num_labels = DocumentSentimentDataset.NUM_LABELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CFn5s0kmuEa"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccHtLgYQXEc-"
      },
      "outputs": [],
      "source": [
        "!rm -rf ~/.cache/pip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ptod4fh7xlMb"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "g = torch.Generator()\n",
        "g.manual_seed(42)\n",
        "\n",
        "for lr in learning_rate:\n",
        "    for dataset_ratio in dataset_ratios:\n",
        "        print(f\"Training with Learning Rate: {lr}, Dataset Ratio: {dataset_ratio}\")\n",
        "\n",
        "        # Dataset preparation\n",
        "        dataset_directory = f'{base_directory}/Dataset/{dataset_ratio}'\n",
        "        train_dataset_path = f'{dataset_directory}/train_data.csv'\n",
        "        valid_dataset_path = f'{dataset_directory}/validation_data.csv'\n",
        "\n",
        "        train_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)\n",
        "        valid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase=True)\n",
        "\n",
        "        train_loader = DocumentSentimentDataLoader(\n",
        "            dataset=train_dataset, max_seq_len=128, batch_size=32,\n",
        "            num_workers=16, shuffle=False, worker_init_fn=seed_worker, generator=g\n",
        "        )\n",
        "\n",
        "        valid_loader = DocumentSentimentDataLoader(\n",
        "            dataset=valid_dataset, max_seq_len=128, batch_size=32,\n",
        "            num_workers=16, shuffle=False, worker_init_fn=seed_worker, generator=g\n",
        "        )\n",
        "\n",
        "        w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
        "        print(w2i)\n",
        "        print(i2w)\n",
        "\n",
        "        # Model initialization\n",
        "        set_seed(42)  # ulangi set seed agar konsisten sebelum from_pretrained\n",
        "        model = BertForSequenceClassification.from_pretrained(f'indobenchmark/{model_name}', config=config)\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "        model = model.cuda()\n",
        "\n",
        "        max_epoch = 20\n",
        "        patience = 3\n",
        "        early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "        for epoch in range(max_epoch):\n",
        "            list_hyp, list_label, list_confidence_train, total_train_loss = train_model(model, train_loader, optimizer, epoch, i2w)\n",
        "            train_accuracy = accuracy_score(list_label, list_hyp)\n",
        "            generate_reports(\n",
        "                train_dataset_path, list_hyp, list_label, list_confidence_train,\n",
        "                dataset_ratio, lr, epoch + 1, train_accuracy, total_train_loss, 'train'\n",
        "            )\n",
        "\n",
        "            list_hyp_val, list_label_val, list_confidence_val, total_loss_val = evaluate_model(model, valid_loader, i2w)\n",
        "            val_accuracy = accuracy_score(list_label_val, list_hyp_val)\n",
        "            generate_reports(\n",
        "                valid_dataset_path, list_hyp_val, list_label_val, list_confidence_val,\n",
        "                dataset_ratio, lr, epoch + 1, val_accuracy, total_loss_val, 'evaluation'\n",
        "            )\n",
        "\n",
        "            early_stopping(total_loss_val, model)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "        list_hyp_test, list_label_test, list_confidence, total_loss_test = test_model(model, valid_loader, i2w)\n",
        "        accuracy = accuracy_score(list_label_test, list_hyp_test)\n",
        "\n",
        "        generate_reports(\n",
        "            valid_dataset_path, list_hyp_test, list_label_test, list_confidence,\n",
        "            dataset_ratio, lr, epoch + 1, accuracy, total_loss_test, 'test',  rename_folder=True\n",
        "        )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8gIcyYilrkLj"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}